#include "legion.h"
#include "taco_legion_header.h"
#include "hdf5_utils.h"
#include "realm/cmdline.h"

using namespace Legion;

typedef double valType;

void registerTacoTasks();

// Forward declarations for partitioning and computation.
struct partitionPackForcomputeLegion;
partitionPackForcomputeLegion* partitionForcomputeLegion(Context ctx, Runtime* runtime, LegionTensor* a, LegionTensor* B, LegionTensor* c);
void computeLegion(Context ctx, Runtime* runtime, LegionTensor* a, LegionTensor* B, LegionTensor* c, partitionPackForcomputeLegion* partitionPack);

// Packing function from COO to CSR.
// TODO (rohany): This is handwritten right now, but in the near future it should be a function that
//  is generated by DISTAL.
void packCOOtoCSR(Context ctx, Runtime* runtime, LegionTensor& a, LegionTensor& b) {

  // Map all regions into physical regions. We need to do this because we haven't launched
  // a task to do so.
  // TODO (rohany): I don't think that I have the infrastructure to do this, but if this
  //  stuff turns out to be too much work, then I could imagine inserting an extra index
  //  variable with extent 1, and then putting the communication for both tensors underneath
  //  that operation to get the mechanisms for task creation etc to work. I think that is a
  //  big hacky though and it might be better to just understand how to physically map regions
  //  when they are needed.

  // TODO (rohany): Do these coord pointers need to be int32_t's or int64_t's?
  typedef FieldAccessor<READ_ONLY,int32_t,1,coord_t,Realm::AffineAccessor<int32_t,1,coord_t>> RAccessorI;
  typedef FieldAccessor<READ_ONLY,valType,1,coord_t,Realm::AffineAccessor<valType,1,coord_t>> RAccessorD;

  typedef FieldAccessor<READ_WRITE,int32_t,1,coord_t,Realm::AffineAccessor<int32_t,1,coord_t>> AccessorI;
  typedef FieldAccessor<READ_WRITE,Rect<1>,1,coord_t,Realm::AffineAccessor<Rect<1>,1,coord_t>> AccessorR;
  typedef FieldAccessor<READ_WRITE,valType,1,coord_t,Realm::AffineAccessor<valType,1,coord_t>> AccessorD;

  // Unpack a's regions.
  auto a2_pos_log = a.indices[1][0];
  auto a2_crd_log = a.indices[1][1];
  auto a2_pos_parent = a.indicesParents[1][0];
  auto a2_crd_parent = a.indicesParents[1][1];
  auto a_vals_log = a.vals;
  auto a_vals_parent = a.valsParent;

  // Unpack B's regions.
  // TODO (rohany): I don't think that I should have a b1_pos here. When actually generating code
  //  I think that I will want an LgSingleton format, or an LgCOO format to construct these things
  //  more easily / avoid surprises with the formats like this.
  auto b1_crd_log = b.indices[0][0];
  auto b1_crd_parent = b.indicesParents[0][0];
  auto b2_crd_log = b.indices[1][0];
  auto b2_crd_parent = b.indicesParents[1][0];
  auto b_vals_log = b.vals;
  auto b_vals_parent = b.valsParent;

  // Malloc each of the regions.
  // TODO (rohany): Figure out how to do the initial allocations for A's regions.

  // This pos array has a definite size, so we should be able to malloc it right away.
  auto a2_pos_phys = legionMalloc(ctx, runtime, a2_pos_log, a2_pos_parent, FID_RECT_1);
  AccessorR a2_pos(a2_pos_phys, FID_RECT_1);
  for (int i = 0; i < a.dims[0]; i++) {
    a2_pos[i] = Rect<1>(0, 0);
  }

  // TODO (rohany): Note the use of the parents here to allocate from rather than the base regions.
  // Perform initial allocations of the crd and values arrays.
  int32_t a2_crd_size = 1000000;
  auto a2_crd_phys = legionMalloc(ctx, runtime, a2_crd_parent, a2_crd_size, FID_COORD);
  AccessorI a2_crd(a2_crd_phys, FID_COORD);
  int32_t ja = 0;
  int32_t a_capacity = 1000000;
  auto a_vals_phys = legionMalloc(ctx, runtime, a_vals_parent, a_capacity, FID_VAL);
  AccessorD a_vals(a_vals_phys, FID_VAL);

  // Allocating B's regions are pretty easy.
  auto b1_crd_phys = legionMalloc(ctx, runtime, b1_crd_log, b1_crd_parent, FID_COORD);
  auto b2_crd_phys = legionMalloc(ctx, runtime, b2_crd_log, b2_crd_parent, FID_COORD);
  auto b_vals_phys = legionMalloc(ctx, runtime, b_vals_log, b_vals_parent, FID_VAL);

  // We need accessors for all of the regions now.
  RAccessorI b1_crd(b1_crd_phys, FID_COORD);
  RAccessorI b2_crd(b2_crd_phys, FID_COORD);
  RAccessorD b_vals(b_vals_phys, FID_VAL);

  // TODO (rohany): This encoding should be different than the standard TACO singleton
  //  as we don't want to just look at this single region holding the start and end
  //  (or maybe we do and that's OK), but for now let's just look at the index space
  //  domains.
  auto ib = 0;
  auto pb1_end = runtime->get_index_space_domain(ctx, b_vals_log.get_index_space()).get_rect<1>().hi[0];

  while (ib < pb1_end) {
    int32_t i = b1_crd[ib];
    int32_t b1_segend = ib + 1;
    while (b1_segend < pb1_end && b1_crd[b1_segend] == i) {
      b1_segend++;
    }
    int32_t pa2_begin = ja;

    for (int32_t jb = ib; jb < b1_segend; jb++) {
      int32_t j = b2_crd[jb];
      if (a_capacity <= ja) {
        a_vals_phys = legionRealloc(ctx, runtime, a_vals_parent, a_vals_phys, a_capacity * 2, FID_VAL);
        a_vals = AccessorD(a_vals_phys, FID_VAL);
        a_capacity *= 2;
      }
      a_vals[Point<1>(ja)] = b_vals[Point<1>(jb)];
      if (a2_crd_size <= ja) {
        a2_crd_phys = legionRealloc(ctx, runtime, a2_crd_parent, a2_crd_phys, a2_crd_size * 2, FID_COORD);
        a2_crd = AccessorI(a2_crd_phys, FID_COORD);
        a2_crd_size *= 2;
      }
      a2_crd[ja] = j;
      ja++;
    }

    a2_pos[i].hi = (ja - pa2_begin) - 1;
    ib = b1_segend;
  }

  int64_t csa2 = 0;
  for (int64_t pa20 = 0; pa20 < 10; pa20++) {
    int64_t numElemsa2 = a2_pos[pa20].hi;
    // TODO (rohany): Note that we can't lower this into a compound addition, due to a Realm
    //  typing limitation.
    a2_pos[pa20].lo = a2_pos[pa20].lo + csa2;
    a2_pos[pa20].hi = a2_pos[pa20].hi + csa2;
    // TODO (rohany): I don't think the current format code generates the addition here, instead
    //  it generates an assignment which isn't correct.
    csa2 += numElemsa2 + 1;
  }


  // Do some final cleanup on the LegionTensors here.
  a.order = b.order;
  a.dims = b.dims;
  // TODO (rohany): This pos subregion doesn't need to be updated, as it's full?
  // TODO (rohany): How do we know what the final dimension size of the region is? It seems like we need
  //  a separate accumulator for it.
  a.indices[1][1] = getSubRegion(ctx, runtime, a.indicesParents[1][1], {0, pb1_end - 1});
  a.vals = getSubRegion(ctx, runtime, a.valsParent, {0, pb1_end - 1});
  // TODO (rohany): The dense format runs index spaces don't need to be updated here.

  // Important: unmap all physical regions (should use a resource collector for this?).
  runtime->unmap_region(ctx, a2_pos_phys);
  runtime->unmap_region(ctx, a2_crd_phys);
  runtime->unmap_region(ctx, a_vals_phys);

  runtime->unmap_region(ctx, b1_crd_phys);
  runtime->unmap_region(ctx, b2_crd_phys);
  runtime->unmap_region(ctx, b_vals_phys);
}

void top_level_task(const Task* task, const std::vector<PhysicalRegion>& regions, Context ctx, Runtime* runtime) {
  std::string cooFileName;
  bool dump = false;
  Realm::CommandLineParser parser;
  parser.add_option_string("-coofile", cooFileName);
  parser.add_option_bool("-dump", dump);
  auto args = Runtime::get_input_args();
  assert(parser.parse_command_line(args.argc, args.argv));
  assert(!cooFileName.empty());

  // Read in our COO matrix.
  auto coo = loadCOOFromHDF5(ctx, runtime, cooFileName, FID_COORD, sizeof(int32_t), FID_VAL, sizeof(valType));

  auto y = createDenseTensor<1, valType>(ctx, runtime, {10}, FID_VAL);
  auto x = createDenseTensor<1, valType>(ctx, runtime, {10}, FID_VAL);
  runtime->fill_field(ctx, y.vals, y.valsParent, FID_VAL, valType(0));
  runtime->fill_field(ctx, x.vals, x.valsParent, FID_VAL, valType(1));

  // Initialize x.
  // TODO (rohany): Extract this into a task so that it can be reused.
  {
    typedef FieldAccessor<WRITE_ONLY,double,1,coord_t,Realm::AffineAccessor<double,1,coord_t>> AccessorD;
    auto x_phys = legionMalloc(ctx, runtime, x.vals, x.valsParent, FID_VAL);
    AccessorD xAcc(x_phys, FID_VAL);
    for (size_t i = 0; i < size_t(x.dims[0]); i++) {
      xAcc[i] = i;
    }
    runtime->unmap_region(ctx, x_phys);
  }

  // Pack the COO matrix into A.
  // TODO (rohany): In the future, I think I want this to a be two step process where we
  //  preprocess the COO file into an HDF5 file that describes exactly the CSR tensor. Then,
  //  to actually run the program we read in the CSR version of the tensor.
  auto A = createSparseTensorForPack<valType>(ctx, runtime, {Dense, Sparse}, {10, 10}, FID_RECT_1, FID_COORD, FID_VAL);

  packCOOtoCSR(ctx, runtime, A, coo);

  // Partition the tensors.
  auto pack = partitionForcomputeLegion(ctx, runtime, &y, &A, &x);
  // TODO (rohany): Benchmark this computation.
  computeLegion(ctx, runtime, &y, &A, &x, pack);

  if (dump) {
    auto yreg = legionMalloc(ctx, runtime, y.vals, y.valsParent, FID_VAL);
    FieldAccessor<READ_WRITE,valType,1,coord_t, Realm::AffineAccessor<valType, 1, coord_t>> yrw(yreg, FID_VAL);
    for (int i = 0; i < y.dims[0]; i++) {
      std::cout << yrw[i] << " ";
    }
    std::cout << std::endl;
    runtime->unmap_region(ctx, yreg);
  }

  // Delete the partition pack.
  delete pack;
}

int TID_TOP_LEVEL = 420;
int main(int argc, char** argv) {
  Runtime::set_top_level_task_id(TID_TOP_LEVEL);
  {
    TaskVariantRegistrar registrar(TID_TOP_LEVEL, "top_level");
    registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));
    registrar.set_replicable();
    Runtime::preregister_task_variant<top_level_task>(registrar, "top_level");
  }
  registerHDF5UtilTasks();
  registerTacoTasks();
  return Runtime::start(argc, argv);
}
