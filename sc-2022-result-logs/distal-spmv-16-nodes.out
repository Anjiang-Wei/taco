/g/g15/yadav2/.bashrc: line 1: module: command not found
/g/g15/yadav2/.bashrc: line 2: module: command not found
/g/g15/yadav2/.bashrc: line 3: module: command not found
/g/g15/yadav2/.bashrc: line 6: module: command not found
stty: standard input: Inappropriate ioctl for device
BENCHID++DISTAL++spmv++arabic-2005++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 42.450000 ms
BENCHID++DISTAL++spmv++arabic-2005++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 22.300000 ms
BENCHID++DISTAL++spmv++arabic-2005++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 12.300000 ms
BENCHID++DISTAL++spmv++arabic-2005++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 7.350000 ms
BENCHID++DISTAL++spmv++arabic-2005++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 7.500000 ms
BENCHID++DISTAL++spmv++it-2004++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 84.250000 ms
BENCHID++DISTAL++spmv++it-2004++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 46.900000 ms
BENCHID++DISTAL++spmv++it-2004++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 23.950000 ms
BENCHID++DISTAL++spmv++it-2004++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 15.400000 ms
BENCHID++DISTAL++spmv++it-2004++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 11.450000 ms
BENCHID++DISTAL++spmv++kmer_A2a++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 154.050000 ms
BENCHID++DISTAL++spmv++kmer_A2a++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 77.600000 ms
BENCHID++DISTAL++spmv++kmer_A2a++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 41.600000 ms
BENCHID++DISTAL++spmv++kmer_A2a++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 22.800000 ms
BENCHID++DISTAL++spmv++kmer_A2a++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 12.500000 ms
BENCHID++DISTAL++spmv++kmer_V1r++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 135.600000 ms
BENCHID++DISTAL++spmv++kmer_V1r++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 69.600000 ms
BENCHID++DISTAL++spmv++kmer_V1r++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 38.650000 ms
BENCHID++DISTAL++spmv++kmer_V1r++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 23.200000 ms
BENCHID++DISTAL++spmv++kmer_V1r++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 14.050000 ms
BENCHID++DISTAL++spmv++mycielskian19++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 67.650000 ms
BENCHID++DISTAL++spmv++mycielskian19++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 46.650000 ms
BENCHID++DISTAL++spmv++mycielskian19++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 32.450000 ms
BENCHID++DISTAL++spmv++mycielskian19++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 22.250000 ms
BENCHID++DISTAL++spmv++mycielskian19++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 16.450000 ms
BENCHID++DISTAL++spmv++nlpkkt240++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 47.250000 ms
BENCHID++DISTAL++spmv++nlpkkt240++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 23.750000 ms
BENCHID++DISTAL++spmv++nlpkkt240++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 12.050000 ms
BENCHID++DISTAL++spmv++nlpkkt240++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 6.450000 ms
BENCHID++DISTAL++spmv++nlpkkt240++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 7.600000 ms
BENCHID++DISTAL++spmv++sk-2005++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 124.950000 ms
BENCHID++DISTAL++spmv++sk-2005++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 63.000000 ms
BENCHID++DISTAL++spmv++sk-2005++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 33.900000 ms
BENCHID++DISTAL++spmv++sk-2005++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 18.350000 ms
BENCHID++DISTAL++spmv++sk-2005++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 12.500000 ms
BENCHID++DISTAL++spmv++twitter7++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1097.650000 ms
BENCHID++DISTAL++spmv++twitter7++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 829.700000 ms
BENCHID++DISTAL++spmv++twitter7++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 597.600000 ms
BENCHID++DISTAL++spmv++twitter7++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 366.250000 ms
BENCHID++DISTAL++spmv++twitter7++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 231.400000 ms
BENCHID++DISTAL++spmv++uk-2005++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 77.350000 ms
BENCHID++DISTAL++spmv++uk-2005++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 45.350000 ms
BENCHID++DISTAL++spmv++uk-2005++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 22.950000 ms
BENCHID++DISTAL++spmv++uk-2005++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 14.200000 ms
BENCHID++DISTAL++spmv++uk-2005++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 7.850000 ms
BENCHID++DISTAL++spmv++webbase-2001++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 113.800000 ms
BENCHID++DISTAL++spmv++webbase-2001++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 57.250000 ms
BENCHID++DISTAL++spmv++webbase-2001++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 31.100000 ms
BENCHID++DISTAL++spmv++webbase-2001++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 17.000000 ms
BENCHID++DISTAL++spmv++webbase-2001++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmv -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 110G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -csr /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 10.600000 ms
logout

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3217337: <../scripts/spbenchmark.py DISTAL spmv all-matrices --nodes 1 2 4 8 16> in cluster <lassen> Done

Job <../scripts/spbenchmark.py DISTAL spmv all-matrices --nodes 1 2 4 8 16> was submitted from host <lassen735> by user <yadav2> in cluster <lassen> at Wed Feb  2 00:00:17 2022
Job was executed on host(s) <1*lassen710>, in queue <pbatch>, as user <yadav2> in cluster <lassen> at Wed Feb  2 00:00:19 2022
                            <40*lassen407>
                            <40*lassen251>
                            <40*lassen737>
                            <40*lassen588>
                            <40*lassen413>
                            <40*lassen415>
                            <40*lassen266>
                            <40*lassen422>
                            <40*lassen752>
                            <40*lassen425>
                            <40*lassen426>
                            <40*lassen755>
                            <40*lassen427>
                            <40*lassen271>
                            <40*lassen428>
                            <40*lassen272>
</g/g15/yadav2> was used as the home directory.
</g/g15/yadav2/taco/build> was used as the working directory.
Started at Wed Feb  2 00:00:19 2022
Terminated at Wed Feb  2 00:10:38 2022
Results reported at Wed Feb  2 00:10:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
../scripts/spbenchmark.py DISTAL spmv all-matrices --nodes 1 2 4 8 16
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4.00 sec.
    Max Memory :                                 62 MB
    Average Memory :                             60.85 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1426 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   619 sec.
    Turnaround time :                            621 sec.

The output (if any) is above this job summary.

