/g/g15/yadav2/.bashrc: line 1: module: command not found
/g/g15/yadav2/.bashrc: line 2: module: command not found
/g/g15/yadav2/.bashrc: line 3: module: command not found
/g/g15/yadav2/.bashrc: line 6: module: command not found
stty: standard input: Inappropriate ioctl for device
BENCHID++DISTAL++spmm++arabic-2005++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 598.700000 ms
BENCHID++DISTAL++spmm++arabic-2005++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 314.550000 ms
BENCHID++DISTAL++spmm++arabic-2005++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 171.650000 ms
BENCHID++DISTAL++spmm++arabic-2005++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 90.300000 ms
BENCHID++DISTAL++spmm++arabic-2005++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/arabic-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 50.900000 ms
BENCHID++DISTAL++spmm++it-2004++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1172.800000 ms
BENCHID++DISTAL++spmm++it-2004++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 667.900000 ms
BENCHID++DISTAL++spmm++it-2004++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 338.050000 ms
BENCHID++DISTAL++spmm++it-2004++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 228.950000 ms
BENCHID++DISTAL++spmm++it-2004++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/it-2004.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 176.050000 ms
BENCHID++DISTAL++spmm++kmer_A2a++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1537.050000 ms
BENCHID++DISTAL++spmm++kmer_A2a++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 768.300000 ms
BENCHID++DISTAL++spmm++kmer_A2a++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 382.550000 ms
BENCHID++DISTAL++spmm++kmer_A2a++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 194.700000 ms
BENCHID++DISTAL++spmm++kmer_A2a++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_A2a.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 96.900000 ms
BENCHID++DISTAL++spmm++kmer_V1r++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1564.900000 ms
BENCHID++DISTAL++spmm++kmer_V1r++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 826.100000 ms
BENCHID++DISTAL++spmm++kmer_V1r++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 418.150000 ms
BENCHID++DISTAL++spmm++kmer_V1r++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 221.600000 ms
BENCHID++DISTAL++spmm++kmer_V1r++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/kmer_V1r.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 128.000000 ms
BENCHID++DISTAL++spmm++mycielskian19++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1697.850000 ms
BENCHID++DISTAL++spmm++mycielskian19++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1208.600000 ms
BENCHID++DISTAL++spmm++mycielskian19++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 862.850000 ms
BENCHID++DISTAL++spmm++mycielskian19++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 673.800000 ms
BENCHID++DISTAL++spmm++mycielskian19++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/mycielskian19.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 608.850000 ms
BENCHID++DISTAL++spmm++nlpkkt240++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 799.050000 ms
BENCHID++DISTAL++spmm++nlpkkt240++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 406.400000 ms
BENCHID++DISTAL++spmm++nlpkkt240++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 206.250000 ms
BENCHID++DISTAL++spmm++nlpkkt240++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 102.750000 ms
BENCHID++DISTAL++spmm++nlpkkt240++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/nlpkkt240.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 51.100000 ms
BENCHID++DISTAL++spmm++sk-2005++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1829.850000 ms
BENCHID++DISTAL++spmm++sk-2005++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 920.350000 ms
BENCHID++DISTAL++spmm++sk-2005++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 496.600000 ms
BENCHID++DISTAL++spmm++sk-2005++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 270.150000 ms
BENCHID++DISTAL++spmm++sk-2005++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/sk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 172.450000 ms
BENCHID++DISTAL++spmm++twitter7++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 11225.300000 ms
BENCHID++DISTAL++spmm++twitter7++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 8501.050000 ms
BENCHID++DISTAL++spmm++twitter7++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 6133.800000 ms
BENCHID++DISTAL++spmm++twitter7++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 3024.900000 ms
BENCHID++DISTAL++spmm++twitter7++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/twitter7.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1832.250000 ms
BENCHID++DISTAL++spmm++uk-2005++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1023.300000 ms
BENCHID++DISTAL++spmm++uk-2005++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 617.400000 ms
BENCHID++DISTAL++spmm++uk-2005++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 308.600000 ms
BENCHID++DISTAL++spmm++uk-2005++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 186.200000 ms
BENCHID++DISTAL++spmm++uk-2005++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/uk-2005.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 102.200000 ms
BENCHID++DISTAL++spmm++webbase-2001++1++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 1 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 1159.950000 ms
BENCHID++DISTAL++spmm++webbase-2001++2++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 2 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 579.800000 ms
BENCHID++DISTAL++spmm++webbase-2001++4++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 4 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 317.550000 ms
BENCHID++DISTAL++spmm++webbase-2001++8++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 8 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 170.950000 ms
BENCHID++DISTAL++spmm++webbase-2001++16++nodes
Executing command: jsrun -b none -c ALL_CPUS -g ALL_GPUS -r 1 -n 16 /g/g15/yadav2/taco/build/bin/spmm -ll:ocpu 2 -ll:othr 18 -ll:onuma 1 -ll:nsize 100G -ll:ncsize 0 -ll:util 2 -tm:numa_aware_alloc -n 20 -warmup 10 -tensor /p/gpfs1/yadav2/tensors/distal/webbase-2001.csr.hdf5
Warning: Overriding spectrum-mpi/rolling-release (module loaded must exactly match)
Warning: Using      spectrum-mpi/2020.08.19 to match app's MPI
Please tell John Gyllenhaal (gyllen@llnl.gov, 4-5485) if this MPI env fix doesn't work
Average execution time: 93.450000 ms
logout

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3249953: <../scripts/spbenchmark.py DISTAL spmm all-matrices --nodes 1 2 4 8 16> in cluster <lassen> Done

Job <../scripts/spbenchmark.py DISTAL spmm all-matrices --nodes 1 2 4 8 16> was submitted from host <lassen95> by user <yadav2> in cluster <lassen> at Thu Feb 10 16:07:20 2022
Job was executed on host(s) <1*lassen710>, in queue <pbatch>, as user <yadav2> in cluster <lassen> at Thu Feb 10 17:06:48 2022
                            <40*lassen350>
                            <40*lassen680>
                            <40*lassen507>
                            <40*lassen351>
                            <40*lassen541>
                            <40*lassen542>
                            <40*lassen543>
                            <40*lassen544>
                            <40*lassen545>
                            <40*lassen546>
                            <40*lassen548>
                            <40*lassen550>
                            <40*lassen552>
                            <40*lassen553>
                            <40*lassen555>
                            <40*lassen557>
</g/g15/yadav2> was used as the home directory.
</g/g15/yadav2/taco/build> was used as the working directory.
Started at Thu Feb 10 17:06:48 2022
Terminated at Thu Feb 10 17:50:25 2022
Results reported at Thu Feb 10 17:50:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
../scripts/spbenchmark.py DISTAL spmm all-matrices --nodes 1 2 4 8 16
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4.00 sec.
    Max Memory :                                 62 MB
    Average Memory :                             61.40 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1426 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   2618 sec.
    Turnaround time :                            6185 sec.

The output (if any) is above this job summary.

